{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "ApDi__LX7D0C"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn import tree\n",
    "import random as rand\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oIbBQ-qD31xn"
   },
   "source": [
    "## 1.- Get the MNIST dataset, select one kind of model and train multiple instances of the same model in this dataset (say, from 1 to 100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "4JhilI0w35_P"
   },
   "outputs": [],
   "source": [
    "mnist = fetch_mldata('MNIST original')\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    mnist.data, mnist.target, test_size=0.4, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "SZGN-T3X4C48"
   },
   "outputs": [],
   "source": [
    "#Considering decision tree to be the base algorithm \n",
    "class ensemble():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "#     def train_data_LR(self,train, target):\n",
    "#         n_samples = len(train.data)\n",
    "#         data = train.reshape((n_samples, -1))\n",
    "#         clf = LinearRegression()\n",
    "#         return clf.fit(data, target)\n",
    "    \n",
    "#     def svm(self,train, target):\n",
    "#         n_samples = len(train.data)\n",
    "#         data = train.reshape((n_samples, -1))\n",
    "#         clf = SVC(kernel='rbf', probability=True)\n",
    "#         return clf.fit(data, target)\n",
    "    \n",
    "    def train_data_DT(self, train, target):\n",
    "        n_samples = len(train.data)\n",
    "        data = train.reshape((n_samples, -1))\n",
    "        clf = tree.DecisionTreeClassifier()\n",
    "        return clf.fit(data, target)\n",
    "    \n",
    "    def bagging(self, clf, train, target):\n",
    "        accuracy = []\n",
    "        for i in range(2,4):\n",
    "            print(\"Number of members = \",i)\n",
    "            baglfy=BaggingClassifier(base_estimator=clf,n_estimators=i,max_samples=len(train))\n",
    "            baglfy=baglfy.fit(x_train,y_train)\n",
    "            bag_pred=baglfy.predict(x_test)\n",
    "            print(\"Accuracy = \",accuracy_score(y_test, bag_pred))\n",
    "            accuracy.append(accuracy_score(y_test, bag_pred))\n",
    "        return baglfy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj2 = ensemble()\n",
    "clf2 = obj2.train_data_DT(x_train,y_train)\n",
    "# save the model to disk\n",
    "filename = 'finalized_model.sav'\n",
    "pickle.dump(clf2, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7htBkm23ceaO"
   },
   "source": [
    "## 2.- With the trained models, make ensembles of [2, 3, ..., 100] members, evaluate the testing accuracy of each ensemble, and estimate the uncertainty of their outputs using a histogram or average of softmax outputs (depending on the model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 130
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 85167,
     "status": "ok",
     "timestamp": 1528882527971,
     "user": {
      "displayName": "Pradheep krishna",
      "photoUrl": "//lh5.googleusercontent.com/-M377mmBxoNs/AAAAAAAAAAI/AAAAAAAAALw/wYOSE7fajrY/s50-c-k-no/photo.jpg",
      "userId": "110109036196497630582"
     },
     "user_tz": -120
    },
    "id": "fsadQawAcysl",
    "outputId": "b0c060db-fa5c-448a-dcbb-f439cde5432d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of members =  2\n",
      "Number of members =  3\n"
     ]
    }
   ],
   "source": [
    "n_runs = 4\n",
    "baggingClfs = []\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "for i in range(2,n_runs):\n",
    "    print(\"Number of members = \",i)\n",
    "    baglfy=BaggingClassifier(base_estimator=clf,n_estimators=i,max_samples=len(x_train),bootstrap=True, n_jobs=-1)\n",
    "    baglfy=baglfy.fit(x_train,y_train)\n",
    "    baggingClfs.append(baglfy)\n",
    "    \n",
    "filename = 'finalized_model1.sav'\n",
    "pickle.dump(baggingClfs, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BaggingClassifier(base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'),\n",
      "         bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
      "         max_samples=42000, n_estimators=2, n_jobs=-1, oob_score=False,\n",
      "         random_state=None, verbose=0, warm_start=False), BaggingClassifier(base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'),\n",
      "         bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
      "         max_samples=42000, n_estimators=3, n_jobs=-1, oob_score=False,\n",
      "         random_state=None, verbose=0, warm_start=False)]\n"
     ]
    }
   ],
   "source": [
    "# load the model from disk\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "print(loaded_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncertainity =  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncertainity =  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy = []\n",
    "for i in range(2,n_runs):\n",
    "    uncer = loaded_model.predict_proba(x_train)\n",
    "    print(\"Uncertainity = \", uncer)\n",
    "    print(\"Accuracy = \",accuracy_score(y_test, bag_pred))\n",
    "    accuracy.append(accuracy_score(y_test, bag_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = clf.predict_proba(x_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8ZywLhrr4s3W"
   },
   "source": [
    "## 3.- Does accuracy and uncertainty improve by having more members in each ensemble?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "6I53Vm_w4yrP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iUgRN-ES4zwy"
   },
   "source": [
    "## 4.- Using a single ensemble of your choosing (you define the number of members), find the misclassified examples in the test set and analyze the uncertainty of those examples. Can the uncertainty explain why those examples are misclassified? Give examples and a complete analysis. \n",
    "\n",
    "Intuitively, this can be seen as a simple question. If we have\n",
    "several models trained on the same data, the model's\n",
    "parameters won't be the same. Some parameters might be\n",
    "very close, some will be equal, and some will be radically\n",
    "different.\n",
    "These variations on model parameters can be encoded as a\n",
    "probability distribution. Bayesian statistics can be used to\n",
    "estimate these distributions P(wjD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "dMX4FDLF43bY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XAiB76-444dM"
   },
   "source": [
    "## 5.- Reproduce Figure one from the \"Simple and Scalable Predictive UncertaintyEstimation using Deep Ensembles\" paper (attached) using a random forest, computing regression uncertainty as the variance of the estimates across trees. The authors mention their metodology in Section 3.2, and one can easily reproduce the training set as its a sample of y = x^3 + noise, where noise is Gaussian with zero mean and 3^2 variance, and x is in the range [-4, 4]. The key idea of those figures is to show that uncertainty grows for values outside of the range [-4, 4], more specifically, in the range [-6, -4] and [4, 6]. To produce the figure, you can plot the mean and make confidence bands with size of square root of variance.Remember that I expect a good report of results, not just concentrating on the code, but also on scientific analysis of your results and explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "akSMEmS548OO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "LA_Assignment07.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
